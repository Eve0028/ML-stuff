{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xPZCNLQ2zKO"
      },
      "source": [
        "## Diagnosis of diabetes.\n",
        "\n",
        "- Using data from `sklearn` (`datasets.load_diabetes()`) - reading from a file;\n",
        "\n",
        "- Splitting data into training and validation data in the proportion of 70% / 30%;\n",
        "\n",
        "- Testing different types of networks with one and two hidden layers.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Imports"
      ],
      "metadata": {
        "id": "RyXbpMExmCR6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "roJLXMvG4Dgx"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Input, Dense, Activation\n",
        "from keras.activations import sigmoid, tanh, relu, linear\n",
        "from tensorflow.keras.optimizers import RMSprop, SGD, Adam\n",
        "from keras.initializers import HeNormal, HeUniform, GlorotNormal, GlorotUniform\n",
        "from tensorflow.keras.initializers import random_uniform\n",
        "from keras.losses import BinaryCrossentropy, binary_crossentropy, mse\n",
        "from keras.models import clone_model\n",
        "from sklearn import datasets\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Plot structure"
      ],
      "metadata": {
        "id": "67JJkodFmEi6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ywiPqESeHWTp"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_print(learn_loss, test_loss, test_accur, epoches):\n",
        "    fig = plt.figure()\n",
        "    plt.plot(epoches, learn_loss, label='train_loss')\n",
        "    plt.plot(epoches, test_loss, label='test_loss')\n",
        "    plt.plot(epoches, test_accur, label='test_accur')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    plt.close(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Classes of network models"
      ],
      "metadata": {
        "id": "5hbAfOlEmJCc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "eI4e3tODPtbt"
      },
      "outputs": [],
      "source": [
        "class ModelShape:\n",
        "    def __init__(self, input_shape: int, output_shape: int, hidden_layer_size, activation_input, activation_output,\n",
        "                 initialization: str, learning_rate, loss, optimizer, use_bias: bool = True):\n",
        "        self.input_shape = input_shape\n",
        "        self.output_shape = output_shape\n",
        "        self.hidden_layer_size = hidden_layer_size\n",
        "        self.activation_input = activation_input\n",
        "        self.activation_output = activation_output\n",
        "        self.initialization = initialization\n",
        "        self.learning_rate = learning_rate\n",
        "        self.loss = loss\n",
        "        self.optimizer = optimizer\n",
        "        self.use_bias = use_bias\n",
        "\n",
        "    @staticmethod\n",
        "    def build_model(model, input_shape):\n",
        "        model.build((None, input_shape))\n",
        "\n",
        "    @staticmethod\n",
        "    def compile_model(model, optimizer, learning_rate, loss):\n",
        "        optimizer = optimizer(learning_rate=learning_rate)\n",
        "        model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "    @staticmethod\n",
        "    def get_build_and_compiled_model(model, input_shape, optimizer, learning_rate, loss):\n",
        "        ModelShape.build_model(model, input_shape)\n",
        "        ModelShape.compile_model(model, optimizer, learning_rate, loss)\n",
        "\n",
        "\n",
        "class BaseModel(ModelShape):\n",
        "    def __init__(self, input_shape: int, output_shape: int, hidden_layer_size, activation_input, activation_output,\n",
        "                 initialization: str, learning_rate, loss, optimizer, use_bias: bool = True):\n",
        "        super().__init__(input_shape, output_shape, hidden_layer_size, activation_input, activation_output,\n",
        "                         initialization, learning_rate, loss, optimizer, use_bias)\n",
        "        self.model = None\n",
        "\n",
        "    def create_model(self):\n",
        "        self.model = Sequential()\n",
        "        for hidden_layers_size in self.hidden_layer_size:\n",
        "            self.model.add(Dense(hidden_layers_size, use_bias=self.use_bias,\n",
        "                                 activation=self.activation_input,\n",
        "                                 kernel_initializer=self.initialization,\n",
        "                                 bias_initializer=self.initialization))\n",
        "        self.model.add(Dense(self.output_shape, use_bias=self.use_bias, activation=self.activation_output,\n",
        "                             kernel_initializer=self.initialization,\n",
        "                             bias_initializer=self.initialization))\n",
        "\n",
        "    def show_summary(self):\n",
        "        return self.model.summary()\n",
        "\n",
        "\n",
        "class ActivationInputAndInitModel:\n",
        "    def __init__(self, activation, initialization):\n",
        "        self.initialization = initialization\n",
        "        self.activation = activation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Network training and testing - saving results"
      ],
      "metadata": {
        "id": "WLISMTBRmVXo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Vt44G5mdHg0f"
      },
      "outputs": [],
      "source": [
        "def model_training_epoches(all_models, model_names, test_number, max_epochs, epochs_step, batch_size, data_learn_x,\n",
        "                           data_learn_y, data_test_x, data_test_y, file_name=\"data_models_training.txt\"):\n",
        "\n",
        "    models_scores = {}\n",
        "    for model, model_name in zip(all_models, model_names):\n",
        "        # val_loss_model = []\n",
        "        learn_loss_all = []\n",
        "        test_loss_all = []\n",
        "        test_accur_all = []\n",
        "        epochs_all = [epoc for epoc in range(0, max_epochs + 1, epochs_step)]\n",
        "        min_test_loss = []\n",
        "        max_result_all = []\n",
        "        min_test_epoches = []\n",
        "        stop_training_epoches = {}\n",
        "        stop_training_epoches_mean = max_epochs\n",
        "\n",
        "        for test_nr in range(test_number):\n",
        "            model_test = clone_model(model.model)\n",
        "            ModelShape.get_build_and_compiled_model(model_test, model.input_shape, model.optimizer,\n",
        "                                                    model.learning_rate, model.loss)\n",
        "            test_loss_first = 1\n",
        "            # learn_loss_first = 1\n",
        "            test_loss_dec = 0\n",
        "            # bigger_los_difference = 0\n",
        "            learn_loss = []\n",
        "            test_loss = []\n",
        "            test_result = []\n",
        "            for epoch in range(0, max_epochs + 1, epochs_step):\n",
        "                # Training\n",
        "                model_test.fit(data_learn_x, data_learn_y, epochs=epochs_step, verbose=0, batch_size=batch_size)\n",
        "\n",
        "                # Tests and save results\n",
        "                learn_loss.append(model_test.evaluate(data_learn_x, data_learn_y, verbose=0))\n",
        "                tst_lss, tst_result = model_test.evaluate(data_test_x, data_test_y, verbose=0)\n",
        "                test_loss.append(tst_lss)\n",
        "                test_result.append(tst_result)\n",
        "                # learn_loss_second = learn_loss\n",
        "\n",
        "                # Stop learning if loss on the test data starts to rise.\n",
        "                # Currently, I only save the epoch when we \"should\" stop learning.\n",
        "                test_loss_second = test_loss[-1]\n",
        "                if test_loss_first < test_loss_second:\n",
        "                    test_loss_dec += 1\n",
        "                # if (test_loss_first - learn_loss_first) / test_loss_first < (test_loss_second - learn_loss_second) / test_loss_second\n",
        "                if epoch > 50 and test_loss_dec == 3:\n",
        "                    stop_training_epoches[test_nr] = epoch\n",
        "                    # break\n",
        "                test_loss_first = test_loss_second\n",
        "\n",
        "            # Plots data\n",
        "            learn_loss_all.append(learn_loss)\n",
        "            test_loss_all.append(test_loss)\n",
        "            test_accur_all.append(test_result)\n",
        "            min_test_loss.append(np.min(np.array(test_loss)))\n",
        "            max_result_all.append(np.max(np.array(test_result)))\n",
        "            min_test_epoches.append(epochs_all[test_loss.index(min_test_loss[-1])])\n",
        "            # val_loss_model.append(test_loss[-1])\n",
        "\n",
        "        # Save results\n",
        "        models_scores[model_name] = [np.mean(min_test_loss), np.mean(max_result_all),\n",
        "                                     np.mean(np.array(min_test_epoches))]\n",
        "        stop_training_epoches_mean = np.mean(np.array(list(stop_training_epoches.values())))\n",
        "\n",
        "        # Calculate plots data\n",
        "        learn_loss_to_plot = np.mean(np.array(learn_loss_all), axis=0)\n",
        "        test_loss_to_plot = np.mean(np.array(test_loss_all), axis=0)\n",
        "        test_accur_to_plot = np.mean(np.array(test_accur_all), axis=0)\n",
        "        print(\"MODEL:\")\n",
        "        print(model_name)\n",
        "        print(\"Average model results: \\n\")\n",
        "        plot_print(learn_loss_to_plot[:, 0].tolist(), test_loss_to_plot.tolist(),\n",
        "                   test_accur_to_plot.tolist(), epochs_all[:len(test_loss_to_plot)])\n",
        "        print(f'On average, early stopping occured at epoch: {stop_training_epoches_mean}')\n",
        "        print(\"Results [best_test_loss, best_test_accurate, best_epoch]:\")\n",
        "        print(f'{models_scores[model_name]}\\n')\n",
        "\n",
        "        print(\"\\nThe last training and results of the model: \\n\")\n",
        "        plot_print((np.array(learn_loss)[:, 0]).tolist(), test_loss,\n",
        "                   test_result, epochs_all[:len(test_loss_to_plot)])\n",
        "        print(\"Results [best_test_loss, best_test_accurate, best_epoch]:\")\n",
        "        print(f'[{min_test_loss[-1]}, {max_result_all[-1]}, {min_test_epoches[-1]}]')\n",
        "        if test_number - 1 in stop_training_epoches:\n",
        "            print(f'Early stopping occured at epoch: {stop_training_epoches[test_number - 1]}\\n')\n",
        "        print('\\n\\n')\n",
        "\n",
        "    return models_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Prepare data for training and testing"
      ],
      "metadata": {
        "id": "TWk0tL7sl57u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "WEuKmILY0DOv"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('pima-indians-diabetes.data.csv')\n",
        "data_learn_count = int(0.7 * len(data))\n",
        "data_learn = np.asarray(data[:data_learn_count])\n",
        "data_learn_x = data_learn[:, :-1]\n",
        "data_learn_y = data_learn[:, -1:]\n",
        "\n",
        "data_test = np.asarray(data[data_learn_count:])\n",
        "data_test_x = data_test[:, :-1]\n",
        "data_test_y = data_test[:, -1:]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Creating network models with specific parameters that had the best results during testing - two networks are presented (one single-layer and one two-layer)"
      ],
      "metadata": {
        "id": "789UPvPNmiCW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "fzWr2IWmzoa_"
      },
      "outputs": [],
      "source": [
        "learning_rates = [0.001]\n",
        "\n",
        "hidden_layer_sizes = [[100]]\n",
        "hidden_layers_sizes = [[100, 50]]\n",
        "\n",
        "model_names = []\n",
        "small_models = []\n",
        "big_models = []\n",
        "\n",
        "for learning_rate in learning_rates:\n",
        "    for hidden_layer_size in hidden_layer_sizes:\n",
        "        small_models.append(BaseModel(data_learn_x.shape[1], data_learn_y.shape[1], hidden_layer_size,\n",
        "                                      'relu', 'sigmoid', 'uniform', learning_rate,\n",
        "                                      binary_crossentropy, Adam))\n",
        "        model_names.append(f'hidden_layer_size: {hidden_layer_size}\\n'\n",
        "                           f'activation_input: relu, activation_output: sigmoid,\\n'\n",
        "                           f'initialization: uniform\\n'\n",
        "                           f'learning_rate: {learning_rate}, loss_function: binary_crossentropy,\\n'\n",
        "                           f'optimizer: Adam\\n')\n",
        "        \n",
        "    for hidden_layer_size in hidden_layers_sizes:\n",
        "        big_models.append(BaseModel(data_learn_x.shape[1], data_learn_y.shape[1], hidden_layer_size,\n",
        "                                    'relu', 'sigmoid', 'uniform', learning_rate,\n",
        "                                    binary_crossentropy, Adam))\n",
        "        model_names.append(f'hidden_layer_size: {hidden_layer_size}\\n'\n",
        "                           f'activation_input: relu, activation_output: sigmoid,\\n'\n",
        "                           f'initialization: uniform\\n'\n",
        "                           f'learning_rate: {learning_rate}, loss_function: binary_crossentropy,\\n'\n",
        "                           f'optimizer: Adam\\n')\n",
        "\n",
        "all_models = small_models + big_models"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Main program - parameter initialization with best results"
      ],
      "metadata": {
        "id": "4S2E_Ul4myRJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u1TVfnCszxvM"
      },
      "outputs": [],
      "source": [
        "for model in all_models:\n",
        "    model.create_model()\n",
        "\n",
        "test_number = 10\n",
        "max_epochs = 200\n",
        "# Selection of parameters with the best results in the models.\n",
        "epochs_step = 10\n",
        "batch_size = 5\n",
        "# file_name = f'data_models_training_epoches_100.txt'\n",
        "\n",
        "models_scores = model_training_epoches(all_models, model_names, test_number, max_epochs, epochs_step, batch_size,\n",
        "                                       data_learn_x, data_learn_y, data_test_x, data_test_y)\n",
        "\n",
        "# models_scores = {k: v for k, v in sorted(models_scores.items(), key=lambda item: item[1][0])}\n",
        "# for key, value in models_scores.items():\n",
        "#     print(key)\n",
        "#     print(f'{value}')\n",
        "#     print('\\n\\n')         "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Największa uzyskana wartość accuracy: ok. 78%"
      ],
      "metadata": {
        "id": "rqKgGVfJScxZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Korzystna konfiguracja parametrów do powyższego zadania dla obu typów sieci (z jedną i dwiema warstwami ukrytymi):<br>\n",
        "Wielkość pierwszej warstwy ukrytej: ok. 100 (100 i 50 przy dwuwarstwowej);<br>\n",
        "Funkcja aktywacji wejścia: relu;<br>\n",
        "Funkcja aktywacji wyjścia: sigmoid;<br>\n",
        "Inicjalizacja wag: uniform;<br>\n",
        "Współczynnik uczenia (learning rate): 0.001; <br>\n",
        "Funkcja straty (loss): binary_crossentropy; <br>\n",
        "Optymalizator (optimizer): Adam.<br>\n",
        "\n",
        "batch_size: 5"
      ],
      "metadata": {
        "id": "Z45iAb_iocc5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Wielkość warstw:<br>\n",
        "Przy zwiększaniu ilości neuronów w warstwach w sieciach o dwóch warstwach ukrytych (od 100 w górę przy pierwszej warstwie - kolejna dwa razy mniejsza) wyniki nie polepszały się (jedynie najlepsze wyniki pojawiały się kilkanaście epok wcześniej). W sieciach o jednej warstwie ukrytej im więcej neuronów w warstwie (od 100 w górę) tym gorsze wyniki.<br><br>\n",
        "\n",
        "#### Funkcje aktywacji:<br>\n",
        "ReLU pomaga przy poradzeniu sobie z problemem znikającego gradientu (zanikania błędu przy niższych warstwach, co znacznie pogarsza szybkość uczenia). Jest dużo bardziej efektywna przy większej ilości warstw ukrytych. Mimo to używanie tanh jako funkcji aktywacji warstw wejściowych (ukrytych) nie odbiega wynikami znacznie od wyników relu (przy jednej i dwóch warstwach).<bt>\n",
        "\n",
        "Sigmoid jako funkcja aktywacji warstwy wyjściowej pasuje do problemu powyższego zadania (binarnej klasyfikacji) - dzięki przedziałowi wyjściowemu 0 - 1.<br><br>\n",
        "\n",
        "#### Inicjalizacja wag:<br>\n",
        "Tutaj było sporo testów. Podobno HeNormal dobrze zgrywa się z funkcją aktywacji ReLU - w tym przypadku jednak wygrał uniform.<br><br>\n",
        "\n",
        "#### Współczynnik uczenia:<br>\n",
        "0.001 wydało się odpowiednią wartością (przy 0.01 wykresy skakały, przy 0.0001 uczenie było wolne bez znaczącej poprawy). Można jeszcze potestować wartości pośrednie między 0.001 a 0.0001.<br><br>\n",
        "\n",
        "#### Funkcja straty (loss):<br>\n",
        "Binary_crossentropy jest odpowiednia do zadań z binarną klasyfikacją.<br><br>\n",
        "\n",
        "#### Optymalizator sieci (optimizer):<br>\n",
        "Kilka testów. Adam był jednym z lepszych.<br><br>\n",
        "\n",
        "#### Porównanie typów sieci:\n",
        "Spodziewałam się, że sieć z dwoma warstwami ukrytymi poradzi sobie lepiej od sieci z jedną warstwą. Wyniki są porównywalne (dla sieci dwuwarstwowej: 100, 50 i jednowarstwowej: 100), mimo to sieć jednowarstwowa jest trochszeczkę lepsza (w wartości straty i dokładności przewidywania). Pokazuje to, że sieci o mniejszej liczbie warst mogą być w określonych przypadkach problemów lepsze od sieci o większej ich ilości (plus szybsze w uczeniu).<br><br>\n",
        "\n",
        "#### Przerwanie uczenia:\n",
        "Przerwać uczenie powinniśmy gdy wartość strat (loss) dla danych testowych zaczyna rosnąć (zamisat maleć) - przy pojawieniu się przeuczenia sieci. Trudno jednoznacznie powiedzieć kiedy to następuje - funkcja strat dla danych testowych może kilka razy wzrosnąć przed kolejnym spadkiem (możemy zatrzymac uczenie zbyt wcześnie). <br>\n",
        "Można:<br>\n",
        "- sprawdzać ile razy funkcja maleje (porównywać loss wcześniejszy i aktualny) - przy którymś razie zatrzymać uczenie,<br>\n",
        "- sprawdzać czy loss aktualny jest większy od poprzedniego (np. określoną ilość epok wcześniej), jeśli tak - przerwać uczenie jesli współczynnik actual_loss/previous_loss będzie odpowiednio duży.<br>\n",
        "- ułożyć kombinacje różnych sposobów.<br>\n",
        "\n",
        "Zastosowany został pierwszy z wymienionych sposobów."
      ],
      "metadata": {
        "id": "PDTbXvj2yB9K"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}